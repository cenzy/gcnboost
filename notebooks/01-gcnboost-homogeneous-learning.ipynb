{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jbananafish/Desktop/Master/Thesis/code/gcnboost\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, Linear, GraphConv, GATConv, to_hetero\n",
    "import torch_geometric.transforms as T\n",
    "import torch_geometric.nn as operators\n",
    "\n",
    "from src.data.artgraph import ArtGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_data = ArtGraph(\"./ekg\", preprocess='node2vec', features=True, type='ekg')\n",
    "base_data = ArtGraph(\"data\", preprocess='node2vec', transform=T.ToUndirected(), features=True, type='ekg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = base_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some graph-level information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of artist classes: 300\n",
      "Number of style classes: 83\n",
      "Number of genre classes: 50\n",
      "Number of input features: 128\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of artist classes: {base_data.num_classes['artist']}\")\n",
    "print(f\"Number of style classes: {base_data.num_classes['style']}\")\n",
    "print(f\"Number of genre classes: {base_data.num_classes['genre']}\")\n",
    "print(f\"Number of input features: {base_data.num_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some node-level information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = base_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1martwork\u001b[0m={\n",
      "    x=[61477, 128],\n",
      "    y_artist=[61477],\n",
      "    y_style=[61477],\n",
      "    y_genre=[61477],\n",
      "    train_mask=[61477],\n",
      "    val_mask=[61477],\n",
      "    test_mask=[61477]\n",
      "  },\n",
      "  \u001b[1martist\u001b[0m={ x=[300, 128] },\n",
      "  \u001b[1mgallery\u001b[0m={ x=[1090, 128] },\n",
      "  \u001b[1mcity\u001b[0m={ x=[665, 128] },\n",
      "  \u001b[1mcountry\u001b[0m={ x=[64, 128] },\n",
      "  \u001b[1mstyle\u001b[0m={ x=[83, 128] },\n",
      "  \u001b[1mperiod\u001b[0m={ x=[53, 128] },\n",
      "  \u001b[1mgenre\u001b[0m={ x=[50, 128] },\n",
      "  \u001b[1mserie\u001b[0m={ x=[610, 128] },\n",
      "  \u001b[1mauction\u001b[0m={ x=[5, 128] },\n",
      "  \u001b[1mtag\u001b[0m={ x=[5146, 128] },\n",
      "  \u001b[1mmedia\u001b[0m={ x=[160, 128] },\n",
      "  \u001b[1msubject\u001b[0m={ x=[2161, 128] },\n",
      "  \u001b[1mtraining_node\u001b[0m={ x=[108, 128] },\n",
      "  \u001b[1mfield\u001b[0m={ x=[65, 128] },\n",
      "  \u001b[1mmovement\u001b[0m={ x=[121, 128] },\n",
      "  \u001b[1mpeople\u001b[0m={ x=[48, 128] },\n",
      "  \u001b[1m(artist, influenced_rel, artist)\u001b[0m={ edge_index=[2, 62] },\n",
      "  \u001b[1m(artist, subject_rel, subject)\u001b[0m={ edge_index=[2, 3648] },\n",
      "  \u001b[1m(artist, training_rel, training_node)\u001b[0m={ edge_index=[2, 130] },\n",
      "  \u001b[1m(artist, field_rel, field)\u001b[0m={ edge_index=[2, 323] },\n",
      "  \u001b[1m(artist, movement_rel, movement)\u001b[0m={ edge_index=[2, 286] },\n",
      "  \u001b[1m(artist, patrons_rel, people)\u001b[0m={ edge_index=[2, 45] },\n",
      "  \u001b[1m(artist, teacher_rel, artist)\u001b[0m={ edge_index=[2, 12] },\n",
      "  \u001b[1m(gallery, city_rel, city)\u001b[0m={ edge_index=[2, 937] },\n",
      "  \u001b[1m(city, country_rel, country)\u001b[0m={ edge_index=[2, 581] },\n",
      "  \u001b[1m(gallery, country_rel, country)\u001b[0m={ edge_index=[2, 6] },\n",
      "  \u001b[1m(artwork, media_rel, media)\u001b[0m={ edge_index=[2, 56347] },\n",
      "  \u001b[1m(artwork, about_rel, tag)\u001b[0m={ edge_index=[2, 186612] },\n",
      "  \u001b[1m(artwork, genre_rel, genre)\u001b[0m={ edge_index=[2, 61820] },\n",
      "  \u001b[1m(artwork, style_rel, style)\u001b[0m={ edge_index=[2, 63157] },\n",
      "  \u001b[1m(artwork, author_rel, artist)\u001b[0m={ edge_index=[2, 61477] },\n",
      "  \u001b[1m(artwork, period_rel, period)\u001b[0m={ edge_index=[2, 5160] },\n",
      "  \u001b[1m(artwork, locatedin_rel, gallery)\u001b[0m={ edge_index=[2, 23508] },\n",
      "  \u001b[1m(artwork, auction_rel, auction)\u001b[0m={ edge_index=[2, 28] },\n",
      "  \u001b[1m(artwork, serie_rel, serie)\u001b[0m={ edge_index=[2, 5152] },\n",
      "  \u001b[1m(artwork, completedin_rel, city)\u001b[0m={ edge_index=[2, 4700] },\n",
      "  \u001b[1m(people, patrons_rel, artist)\u001b[0m={ edge_index=[2, 13] },\n",
      "  \u001b[1m(subject, rev_subject_rel, artist)\u001b[0m={ edge_index=[2, 3648] },\n",
      "  \u001b[1m(training_node, rev_training_rel, artist)\u001b[0m={ edge_index=[2, 130] },\n",
      "  \u001b[1m(field, rev_field_rel, artist)\u001b[0m={ edge_index=[2, 323] },\n",
      "  \u001b[1m(movement, rev_movement_rel, artist)\u001b[0m={ edge_index=[2, 286] },\n",
      "  \u001b[1m(people, rev_patrons_rel, artist)\u001b[0m={ edge_index=[2, 45] },\n",
      "  \u001b[1m(city, rev_city_rel, gallery)\u001b[0m={ edge_index=[2, 937] },\n",
      "  \u001b[1m(country, rev_country_rel, city)\u001b[0m={ edge_index=[2, 581] },\n",
      "  \u001b[1m(country, rev_country_rel, gallery)\u001b[0m={ edge_index=[2, 6] },\n",
      "  \u001b[1m(media, rev_media_rel, artwork)\u001b[0m={ edge_index=[2, 56347] },\n",
      "  \u001b[1m(tag, rev_about_rel, artwork)\u001b[0m={ edge_index=[2, 186612] },\n",
      "  \u001b[1m(genre, rev_genre_rel, artwork)\u001b[0m={ edge_index=[2, 61820] },\n",
      "  \u001b[1m(style, rev_style_rel, artwork)\u001b[0m={ edge_index=[2, 63157] },\n",
      "  \u001b[1m(artist, rev_author_rel, artwork)\u001b[0m={ edge_index=[2, 61477] },\n",
      "  \u001b[1m(period, rev_period_rel, artwork)\u001b[0m={ edge_index=[2, 5160] },\n",
      "  \u001b[1m(gallery, rev_locatedin_rel, artwork)\u001b[0m={ edge_index=[2, 23508] },\n",
      "  \u001b[1m(auction, rev_auction_rel, artwork)\u001b[0m={ edge_index=[2, 28] },\n",
      "  \u001b[1m(serie, rev_serie_rel, artwork)\u001b[0m={ edge_index=[2, 5152] },\n",
      "  \u001b[1m(city, rev_completedin_rel, artwork)\u001b[0m={ edge_index=[2, 4700] },\n",
      "  \u001b[1m(artist, rev_patrons_rel, people)\u001b[0m={ edge_index=[2, 13] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HomoGNN(torch.nn.Module):\n",
    "    def __init__(self, operator=GCNConv, input_channels=128, hidden_channels=16, out_channels=300, num_layers=1, dropout=0.5, skip=False):\n",
    "        super(HomoGNN, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.skip = skip\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        #self.convs.append(operator(input_channels, hidden_channels))\n",
    "        for _ in range(num_layers):\n",
    "            conv = operator(-1, hidden_channels)\n",
    "            lin = Linear(-1, hidden_channels)\n",
    "            self.convs.append(conv)\n",
    "            self.lins.append(lin)\n",
    "        self.conv_out = operator(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            if self.skip:\n",
    "                x = conv(x, edge_index).relu() + self.lins[i](x)\n",
    "            else:\n",
    "                x = conv(x, edge_index).relu()\n",
    "            x = F.dropout(x, self.dropout)\n",
    "        x = self.conv_out(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HomoSGNN(torch.nn.Module):\n",
    "    def __init__(self, operator, input_channels, hidden_channels, out_channels, n_layers, dropout, skip):\n",
    "        super(HomoSGNN, self).__init__()\n",
    "        self.gnn = HomoGNN(operator, input_channels, hidden_channels, out_channels, n_layers, dropout, skip)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return [self.gnn(x, edge_index)]\n",
    "\n",
    "class HomoMGNN(torch.nn.Module):\n",
    "    def __init__(self, operator, input_channels, hidden_channels, out_channels, n_layers, dropout, skip):\n",
    "        super(HomoMGNN, self).__init__()\n",
    "        self.gnn_artist = HomoGNN(operator, input_channels, hidden_channels, out_channels['artist'], n_layers, dropout, skip)\n",
    "        self.gnn_style = HomoGNN(operator, input_channels, hidden_channels, out_channels['style'], n_layers, dropout, skip)\n",
    "        self.gnn_genre = HomoGNN(operator, input_channels, hidden_channels, out_channels['genre'], n_layers, dropout, skip)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return [self.gnn_artist(x, edge_index), self.gnn_style(x, edge_index), self.gnn_genre(x, edge_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtGraphGCNBoost:\n",
    "    operator_registry = {\n",
    "        'SAGEConv': operators.SAGEConv,\n",
    "        'GraphConv': operators.GraphConv,\n",
    "        'GATConv': operators.GATConv,\n",
    "        'GCNConv': operators.GCNConv\n",
    "    }\n",
    "\n",
    "    map_id2labels = {\n",
    "        0: 'artist',\n",
    "        1: 'style',\n",
    "        2: 'genre'\n",
    "    }\n",
    "\n",
    "    map_labels2id = {\n",
    "        'artist': 0,\n",
    "        'style': 1,\n",
    "        'genre': 2\n",
    "    }\n",
    "\n",
    "    def __init__(self, args, training_mode='multi_task'):\n",
    "        \n",
    "        self.traning_mode = training_mode\n",
    "        assert training_mode in ['multi_task', 'single_task']\n",
    "        assert args.operator in self.operator_registry.keys()\n",
    "\n",
    "        self.base_data, self.data, self.y, self.model, self.optimizer = self._bootstrap(args)\n",
    "        self.artworks = self.base_data[0]['artwork']\n",
    "        self.train_mask = self.artworks.train_mask\n",
    "        self.val_mask = self.artworks.val_mask\n",
    "        self.test_mask = self.artworks.test_mask\n",
    "\n",
    "    def _bootstrap(self, args):\n",
    "        base_data = ArtGraph(\"data\", preprocess='node2vec', transform=T.ToUndirected(), features=True, type='ekg')\n",
    "        data = base_data[0]\n",
    "        data = data.to_homogeneous()\n",
    "        if self.traning_mode == 'multi_task':\n",
    "            model = HomoMGNN(operator=self.operator_registry[args.operator],\n",
    "                                input_channels=base_data.num_features,\n",
    "                                hidden_channels=args.hidden,\n",
    "                                out_channels=base_data.num_classes,\n",
    "                                n_layers=args.nlayers,\n",
    "                                dropout=args.dropout,\n",
    "                                skip=args.skip)\n",
    "            y = torch.stack([base_data[0]['artwork'].y_artist, base_data[0]['artwork'].y_style, base_data[0]['artwork'].y_genre])\n",
    "        if self.traning_mode == 'single_task':\n",
    "            model = HomoSGNN(operator=self.operator_registry[args.operator],\n",
    "                                input_channels=base_data.num_features,\n",
    "                                hidden_channels=args.hidden,\n",
    "                                out_channels=base_data.num_classes[args.label],\n",
    "                                n_layers=args.nlayers,\n",
    "                                dropout=args.dropout,\n",
    "                                skip=args.skip)\n",
    "            y = torch.stack([base_data[0]['artwork'][f'y_{args.label}']])\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=3e-4)\n",
    "        \n",
    "        return base_data, data, y, model, optimizer\n",
    "\n",
    "    def get_accuracy(self, predicted, labels):\n",
    "        return predicted.argmax(dim=1).eq(labels).sum()/predicted.shape[0]\n",
    "\n",
    "    def get_accuracies_homo(self, predicted, labels, mask):\n",
    "        size = self.train_mask.shape[0]\n",
    "        accuracies = [] \n",
    "        for i, _ in enumerate(labels):\n",
    "            accuracies.append(self.get_accuracy(predicted[i][:size][mask], labels[i][mask]))\n",
    "        return accuracies\n",
    "\n",
    "    def get_loss(self, predicted, labels):\n",
    "        return F.nll_loss(predicted, labels.type(torch.LongTensor))\n",
    "    \n",
    "    def get_losses_homo(self, predicted, labels, mask):\n",
    "        size = self.train_mask.shape[0]\n",
    "        losses = []\n",
    "        for i, _ in enumerate(labels):\n",
    "            losses.append(self.get_loss(predicted[i][:size][mask], labels[i][mask]))\n",
    "        return losses\n",
    "\n",
    "    def homo_training(self):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        out = self.model(self.data.x, self.data.edge_index)\n",
    "\n",
    "        train_losses = self.get_losses_homo(out, self.y, self.train_mask)\n",
    "        train_total_loss = sum(train_losses)\n",
    "\n",
    "        train_total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        train_accuracies = self.get_accuracies_homo(out, self.y, self.train_mask)\n",
    "\n",
    "        return out, train_losses, train_accuracies\n",
    "\n",
    "    def homo_test(self, out):\n",
    "        val_losses = self.get_losses_homo(out, self.y, self.val_mask)\n",
    "        test_losses = self.get_losses_homo(out, self.y, self.test_mask)\n",
    "\n",
    "        val_accuracies = self.get_accuracies_homo(out, self.y, self.val_mask)\n",
    "        test_accuracies = self.get_accuracies_homo(out, self.y, self.test_mask)\n",
    "\n",
    "        return val_losses, val_accuracies, test_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--exp', type=str, default='default', help='Experiment name.')\n",
    "parser.add_argument('--type', type=str, default='homo', help='Graph type (hetero|homo).')\n",
    "parser.add_argument('--mode', type=str, default='single_task', help='Training mode (multi_task|single_task).')\n",
    "parser.add_argument('--label', type=str, default='artist', help='Label to predict (artist|style|genre).')\n",
    "parser.add_argument('--epochs', type=int, default=1, help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='Initial learning rate.')\n",
    "parser.add_argument('--hidden', type=int, default=16, help='Number of hidden units.')\n",
    "parser.add_argument('--nlayers', type=int, default=1, help='Number of layers.')\n",
    "parser.add_argument('--dropout', type=float, default=0, help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--operator', type=str, default='GCNConv', help='The graph convolutional operator.')\n",
    "parser.add_argument('--aggr', type=str, default='sum', help='Aggregation function.')\n",
    "parser.add_argument('--skip', action='store_true', default='False', help='Add skip connection.')\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn = ArtGraphGCNBoost(args, training_mode=args.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:21<00:00, 21.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artist_train_loss 5.7038\n",
      "artist_train_accuracy 0.0\n",
      "artist_val_loss 5.7025\n",
      "artist_val_accuracy 1.0\n",
      "artist_test_loss 5.7031\n",
      "artist_test_accuracy 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(0, args.epochs)):\n",
    "    out, train_losses, train_accuracies = gcn.homo_training()\n",
    "    val_losses, val_accuracies, test_losses, test_accuracies = gcn.homo_test(out)\n",
    "    if args.mode == 'multi_task':\n",
    "        for i, train_loss_acc in enumerate(zip(train_losses, train_accuracies)):\n",
    "            print(f'{gcn.map_id2labels[i]}_train_loss', round(train_loss_acc[0].detach().item(), 4))\n",
    "            print(f'{gcn.map_id2labels[i]}_train_accuracy', round(train_loss_acc[1].item(), 2) * 100)\n",
    "        for i, val_loss_acc in enumerate(zip(val_losses, val_accuracies)):\n",
    "            print(f'{gcn.map_id2labels[i]}_val_loss', round(val_loss_acc[0].detach().item(), 4))\n",
    "            print(f'{gcn.map_id2labels[i]}_val_accuracy', round(val_loss_acc[1].item(), 2) * 100)\n",
    "        for i, test_loss_acc in enumerate(zip(test_losses, test_accuracies)):\n",
    "            print(f'{gcn.map_id2labels[i]}_test_loss', round(test_loss_acc[0].detach().item(), 4))\n",
    "            print(f'{gcn.map_id2labels[i]}_test_accuracy', round(test_loss_acc[1].item(), 2) * 100)\n",
    "    if args.mode == 'single_task':\n",
    "        print(f'{args.label}_train_loss', round(train_losses[0].detach().item(), 4))\n",
    "        print(f'{args.label}_train_accuracy', round(train_accuracies[0].item(), 2) * 100)\n",
    "        print(f'{args.label}_val_loss', round(val_losses[0].detach().item(), 4))\n",
    "        print(f'{args.label}_val_accuracy', round(val_accuracies[0].item(), 2) * 100)\n",
    "        print(f'{args.label}_test_loss', round(test_losses[0].detach().item(), 4))\n",
    "        print(f'{args.label}_test_accuracy', round(test_accuracies[0].item(), 2) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4cd95a69bfd7a84355187b639ae3e32d243eaa398007ea827fe1dce7201cea5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('thesis-project': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
