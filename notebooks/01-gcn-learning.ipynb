{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jbananafish/Desktop/Master/Thesis/code/gcnboost\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbananafish/anaconda3/envs/thesis-project/lib/python3.9/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at  /opt/conda/conda-bld/pytorch_1631630797748/work/c10/cuda/CUDAFunctions.cpp:115.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, to_hetero\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from src.data.artgraph import ArtGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_data = ArtGraph(\"./ekg\", preprocess='node2vec', features=True, type='ekg')\n",
    "base_data = ArtGraph(\"data\", preprocess='node2vec', transform=T.ToUndirected(), features=True, type='ekg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some graph-level information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of artist classes: 300\n",
      "Number of style classes: 83\n",
      "Number of genre classes: 50\n",
      "Number of input features: 128\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of artist classes: {base_data.num_classes['artist']}\")\n",
    "print(f\"Number of style classes: {base_data.num_classes['style']}\")\n",
    "print(f\"Number of genre classes: {base_data.num_classes['genre']}\")\n",
    "print(f\"Number of input features: {base_data.num_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some node-level information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = base_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1martwork\u001b[0m={\n",
      "    x=[61477, 128],\n",
      "    y_artist=[61477],\n",
      "    y_style=[61477],\n",
      "    y_genre=[61477],\n",
      "    train_mask=[61477],\n",
      "    val_mask=[61477],\n",
      "    test_mask=[61477]\n",
      "  },\n",
      "  \u001b[1martist\u001b[0m={ x=[300, 128] },\n",
      "  \u001b[1mgallery\u001b[0m={ x=[1090, 128] },\n",
      "  \u001b[1mcity\u001b[0m={ x=[665, 128] },\n",
      "  \u001b[1mcountry\u001b[0m={ x=[64, 128] },\n",
      "  \u001b[1mstyle\u001b[0m={ x=[83, 128] },\n",
      "  \u001b[1mperiod\u001b[0m={ x=[53, 128] },\n",
      "  \u001b[1mgenre\u001b[0m={ x=[50, 128] },\n",
      "  \u001b[1mserie\u001b[0m={ x=[610, 128] },\n",
      "  \u001b[1mauction\u001b[0m={ x=[5, 128] },\n",
      "  \u001b[1mtag\u001b[0m={ x=[5146, 128] },\n",
      "  \u001b[1mmedia\u001b[0m={ x=[160, 128] },\n",
      "  \u001b[1msubject\u001b[0m={ x=[2161, 128] },\n",
      "  \u001b[1mtraining_node\u001b[0m={ x=[108, 128] },\n",
      "  \u001b[1mfield\u001b[0m={ x=[65, 128] },\n",
      "  \u001b[1mmovement\u001b[0m={ x=[121, 128] },\n",
      "  \u001b[1mpeople\u001b[0m={ x=[48, 128] },\n",
      "  \u001b[1m(artist, influenced_rel, artist)\u001b[0m={ edge_index=[2, 62] },\n",
      "  \u001b[1m(artist, subject_rel, subject)\u001b[0m={ edge_index=[2, 3648] },\n",
      "  \u001b[1m(artist, training_rel, training_node)\u001b[0m={ edge_index=[2, 130] },\n",
      "  \u001b[1m(artist, field_rel, field)\u001b[0m={ edge_index=[2, 323] },\n",
      "  \u001b[1m(artist, movement_rel, movement)\u001b[0m={ edge_index=[2, 286] },\n",
      "  \u001b[1m(artist, patrons_rel, people)\u001b[0m={ edge_index=[2, 45] },\n",
      "  \u001b[1m(artist, teacher_rel, artist)\u001b[0m={ edge_index=[2, 12] },\n",
      "  \u001b[1m(gallery, city_rel, city)\u001b[0m={ edge_index=[2, 937] },\n",
      "  \u001b[1m(city, country_rel, country)\u001b[0m={ edge_index=[2, 581] },\n",
      "  \u001b[1m(gallery, country_rel, country)\u001b[0m={ edge_index=[2, 6] },\n",
      "  \u001b[1m(artwork, media_rel, media)\u001b[0m={ edge_index=[2, 56347] },\n",
      "  \u001b[1m(artwork, about_rel, tag)\u001b[0m={ edge_index=[2, 186612] },\n",
      "  \u001b[1m(artwork, genre_rel, genre)\u001b[0m={ edge_index=[2, 61820] },\n",
      "  \u001b[1m(artwork, style_rel, style)\u001b[0m={ edge_index=[2, 63157] },\n",
      "  \u001b[1m(artwork, author_rel, artist)\u001b[0m={ edge_index=[2, 61477] },\n",
      "  \u001b[1m(artwork, period_rel, period)\u001b[0m={ edge_index=[2, 5160] },\n",
      "  \u001b[1m(artwork, locatedin_rel, gallery)\u001b[0m={ edge_index=[2, 23508] },\n",
      "  \u001b[1m(artwork, auction_rel, auction)\u001b[0m={ edge_index=[2, 28] },\n",
      "  \u001b[1m(artwork, serie_rel, serie)\u001b[0m={ edge_index=[2, 5152] },\n",
      "  \u001b[1m(artwork, completedin_rel, city)\u001b[0m={ edge_index=[2, 4700] },\n",
      "  \u001b[1m(people, patrons_rel, artist)\u001b[0m={ edge_index=[2, 13] },\n",
      "  \u001b[1m(subject, rev_subject_rel, artist)\u001b[0m={ edge_index=[2, 3648] },\n",
      "  \u001b[1m(training_node, rev_training_rel, artist)\u001b[0m={ edge_index=[2, 130] },\n",
      "  \u001b[1m(field, rev_field_rel, artist)\u001b[0m={ edge_index=[2, 323] },\n",
      "  \u001b[1m(movement, rev_movement_rel, artist)\u001b[0m={ edge_index=[2, 286] },\n",
      "  \u001b[1m(people, rev_patrons_rel, artist)\u001b[0m={ edge_index=[2, 45] },\n",
      "  \u001b[1m(city, rev_city_rel, gallery)\u001b[0m={ edge_index=[2, 937] },\n",
      "  \u001b[1m(country, rev_country_rel, city)\u001b[0m={ edge_index=[2, 581] },\n",
      "  \u001b[1m(country, rev_country_rel, gallery)\u001b[0m={ edge_index=[2, 6] },\n",
      "  \u001b[1m(media, rev_media_rel, artwork)\u001b[0m={ edge_index=[2, 56347] },\n",
      "  \u001b[1m(tag, rev_about_rel, artwork)\u001b[0m={ edge_index=[2, 186612] },\n",
      "  \u001b[1m(genre, rev_genre_rel, artwork)\u001b[0m={ edge_index=[2, 61820] },\n",
      "  \u001b[1m(style, rev_style_rel, artwork)\u001b[0m={ edge_index=[2, 63157] },\n",
      "  \u001b[1m(artist, rev_author_rel, artwork)\u001b[0m={ edge_index=[2, 61477] },\n",
      "  \u001b[1m(period, rev_period_rel, artwork)\u001b[0m={ edge_index=[2, 5160] },\n",
      "  \u001b[1m(gallery, rev_locatedin_rel, artwork)\u001b[0m={ edge_index=[2, 23508] },\n",
      "  \u001b[1m(auction, rev_auction_rel, artwork)\u001b[0m={ edge_index=[2, 28] },\n",
      "  \u001b[1m(serie, rev_serie_rel, artwork)\u001b[0m={ edge_index=[2, 5152] },\n",
      "  \u001b[1m(city, rev_completedin_rel, artwork)\u001b[0m={ edge_index=[2, 4700] },\n",
      "  \u001b[1m(artist, rev_patrons_rel, people)\u001b[0m={ edge_index=[2, 13] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv2 = SAGEConv((-1, -1), out_channels)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, metadata):\n",
    "        super(MGNN, self).__init__()\n",
    "        self.gnn_artist = GNN(hidden_channels, out_channels['artist'])\n",
    "        self.gnn_artist = to_hetero(self.gnn_artist, metadata)\n",
    "\n",
    "        self.gnn_style = GNN(hidden_channels, out_channels['style'])\n",
    "        self.gnn_style = to_hetero(self.gnn_style, metadata)\n",
    "\n",
    "        self.gnn_genre = GNN(hidden_channels, out_channels['genre'])\n",
    "        self.gnn_genre = to_hetero(self.gnn_genre, metadata)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return [self.gnn_artist(x, edge_index), self.gnn_style(x, edge_index), self.gnn_genre(x, edge_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtGraphGCNBoost:\n",
    "\n",
    "    map_labels = {\n",
    "        0: 'artist',\n",
    "        1: 'style',\n",
    "        2: 'genre'\n",
    "    }\n",
    "\n",
    "    def __init__(self, model, data, optimizer):\n",
    "        \n",
    "        self.data = data\n",
    "        self.artworks = data['artwork']\n",
    "        self.y = torch.stack([data['artwork'].y_artist, data['artwork'].y_style, data['artwork'].y_genre])\n",
    "        self.train_mask = self.artworks.train_mask\n",
    "        self.val_mask = self.artworks.val_mask\n",
    "        self.test_mask = self.artworks.test_mask\n",
    "\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def get_classes(self, label= 'artist', split='train'):\n",
    "        pass \n",
    "\n",
    "    def get_accuracy(self, predicted, labels):\n",
    "        return predicted.argmax(dim=1).eq(labels).sum()/predicted.shape[0]\n",
    "\n",
    "    def get_accuracies(self, predicted, labels, mask):\n",
    "        accuracies = [] \n",
    "        for id, _ in self.map_labels.items():\n",
    "            accuracies.append(self.get_accuracy(predicted[id]['artwork'][mask], labels[id][mask]))\n",
    "        return accuracies\n",
    "\n",
    "    def get_loss(self, predicted, labels):\n",
    "        return F.nll_loss(predicted, labels.type(torch.LongTensor))\n",
    "\n",
    "    def get_losses(self, predicted, labels, mask):\n",
    "        losses = []\n",
    "        for id, _ in self.map_labels.items():\n",
    "            losses.append(self.get_loss(predicted[id]['artwork'][mask], labels[id][mask]))\n",
    "        return losses\n",
    "\n",
    "    def train(self, epochs):\n",
    "        for epoch in tqdm(range(0, epochs)):\n",
    "            out = self.multi_task_training(epoch)\n",
    "        return out\n",
    "\n",
    "\n",
    "    def multi_task_training(self, epoch):\n",
    "        self.model.train()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        out = model(self.data.x_dict, self.data.edge_index_dict)\n",
    "        train_losses = self.get_losses(out, self.y, self.train_mask)\n",
    "        train_total_loss = sum(train_losses)\n",
    "        train_total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_accuracies = self.get_accuracies(out, self.y, self.train_mask)\n",
    "\n",
    "        print(f'Epoch: {epoch+1}')\n",
    "        for i, train_loss_acc in enumerate(zip(train_losses, train_accuracies)):\n",
    "            print(f'\\t {self.map_labels[i]} \\t {round(train_loss_acc[0].detach().item(), 4)} \\t{round(train_loss_acc[1].item(), 2) * 100}%')\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            self.test(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def test(self, out):\n",
    "        val_losses = self.get_losses(out, self.y, self.val_mask)\n",
    "        test_losses = self.get_losses(out, self.y, self.test_mask)\n",
    "\n",
    "        val_accuracies = self.get_accuracies(out, self.y, self.val_mask)\n",
    "        test_accuracies = self.get_accuracies(out, self.y, self.test_mask)\n",
    "\n",
    "        print(f'*\\tOn validation')\n",
    "        for i, val_loss_acc in enumerate(zip(val_losses, val_accuracies)):\n",
    "            print(f'\\t{self.map_labels[i]}\\t {round(val_loss_acc[0].detach().item(), 4)} \\t {round(val_loss_acc[1].item(), 2) * 100}%')\n",
    "\n",
    "        print(f'*\\tOn test')\n",
    "        for i, test_loss_acc in enumerate(zip(test_losses, test_accuracies)):\n",
    "            print(f'\\t{self.map_labels[i]}\\t {round(test_loss_acc[0].detach().item(), 4)} \\t {round(test_loss_acc[1].item(), 2) * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MGNN( hidden_channels=16, out_channels=base_data.num_classes, metadata=data.metadata())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "gcn = ArtGraphGCNBoost(model, data, optimizer)\n",
    "out = gcn.train(1000)\n",
    "torch.save(out, \"out.pt\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4cd95a69bfd7a84355187b639ae3e32d243eaa398007ea827fe1dce7201cea5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('thesis-project': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
